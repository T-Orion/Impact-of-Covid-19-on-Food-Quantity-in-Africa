{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expressions\n",
    "import nltk # for text manipulation\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Still Don't know where my dear country Nigeria is heading\\nInsecurity\\nHunger\\nGivernment failures\\njudiciary not trusted\\nHow can the poor survive.\\nGod save Nigeria</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WFP links record hunger levels to pandemic, conflict, climate change</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WFP links record hunger levels to pandemic, conflict, climate change</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>: WFP links record hunger levels to pandemic, conflict, climate change</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>: Hunger in the northeast Nigeria will spike more conflicts      …</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                   tweets  \\\n",
       "0  Still Don't know where my dear country Nigeria is heading\\nInsecurity\\nHunger\\nGivernment failures\\njudiciary not trusted\\nHow can the poor survive.\\nGod save Nigeria   \n",
       "1                                                                                                   WFP links record hunger levels to pandemic, conflict, climate change    \n",
       "2                                                                                                   WFP links record hunger levels to pandemic, conflict, climate change    \n",
       "3                                                                                                 : WFP links record hunger levels to pandemic, conflict, climate change    \n",
       "4                                                                                                      : Hunger in the northeast Nigeria will spike more conflicts      …   \n",
       "\n",
       "   subjectivity  polarity  analysis  \n",
       "0           0.6      -0.4  Negative  \n",
       "1           0.0       0.0   Neutral  \n",
       "2           0.0       0.0   Neutral  \n",
       "3           0.0       0.0   Neutral  \n",
       "4           0.5       0.5  Positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/NG_tweets.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis\n",
    "\n",
    "**Let's Explore what Nigeria Tweets is all about to get a sense of the data if we doing the right thing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4062, 4)\n",
      "(1340, 4)\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "print(df.shape)\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of duplicates values in the tweets more than 70% Now that cleaning damn!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340, 4)\n",
      "(1339, 4)\n"
     ]
    }
   ],
   "source": [
    "# check for missing values if any drop\n",
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like after dropping duplicates we have no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types\n",
      "------------------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1339 entries, 0 to 4052\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   tweets        1339 non-null   object \n",
      " 1   subjectivity  1339 non-null   float64\n",
      " 2   polarity      1339 non-null   float64\n",
      " 3   analysis      1339 non-null   object \n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 52.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# check dtypes of each feature\n",
    "print('Data Types')\n",
    "print('---------'*10)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert analysis column to integer\n",
    "df['analysis'].value_counts().plot(kind='bar', figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign positive to 1, Neutral to 0 and Negative to -1\n",
    "df['analysis'] = df['analysis'].map({'Positive': 1, 'Neutral': 0, 'Negative': -1}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1339 entries, 0 to 4052\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   tweets        1339 non-null   object \n",
      " 1   subjectivity  1339 non-null   float64\n",
      " 2   polarity      1339 non-null   float64\n",
      " 3   analysis      1339 non-null   int32  \n",
      "dtypes: float64(2), int32(1), object(1)\n",
      "memory usage: 47.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization and Stematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ABUTON\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WFP', 'links', 'record', 'hunger', 'levels', 'to', 'pandemic', ',', 'conflict', ',', 'climate', 'change']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split text into words using NLTK\n",
    "words = word_tokenize(str(df['tweets'][1]))\n",
    "print(words)\n",
    "# df['tweets'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ABUTON\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# List stop words\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WFP', 'links', 'record', 'hunger', 'levels', 'pandemic', ',', 'conflict', ',', 'climate', 'change']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wfp', 'link', 'record', 'hunger', 'level', 'pandem', ',', 'conflict', ',', 'climat', 'chang']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ABUTON\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WFP', 'link', 'record', 'hunger', 'level', 'pandemic', ',', 'conflict', ',', 'climate', 'change']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WFP', 'link', 'record', 'hunger', 'level', 'pandemic', ',', 'conflict', ',', 'climate', 'change']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize verbs by specifying pos\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# import gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1339, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df=.90, min_df=2, max_features=1000, stop_words='english')\n",
    "bow_vectorizer = bow_vectorizer.fit_transform(df['tweets'])\n",
    "bow_vectorizer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1339, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF = (Number of times term t appears in a document) / (Number of terms in the document)\n",
    "# IDF = log(N/n), where, N is the number of documents and n is the number of documents a term has appeared in\n",
    "# TF-IDF = TF*IDF\n",
    "\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_df=.90, min_df=2, max_features=1000, stop_words='english')\n",
    "tf_idf = pd.DataFrame(tf_idf_vectorizer.fit_transform(df['tweets']).toarray(), columns=tf_idf_vectorizer.get_feature_names())\n",
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__</th>\n",
       "      <th>_africa</th>\n",
       "      <th>_cakes</th>\n",
       "      <th>_farouq</th>\n",
       "      <th>_food</th>\n",
       "      <th>_fsh</th>\n",
       "      <th>_ja</th>\n",
       "      <th>_ng</th>\n",
       "      <th>_nigeria</th>\n",
       "      <th>_of_lagos</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yoruba</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youths</th>\n",
       "      <th>zero</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zoo</th>\n",
       "      <th>àdìre</th>\n",
       "      <th>églises</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    __  _africa  _cakes  _farouq  _food  _fsh  _ja  _ng  _nigeria  _of_lagos  \\\n",
       "0  0.0      0.0     0.0      0.0    0.0   0.0  0.0  0.0       0.0        0.0   \n",
       "1  0.0      0.0     0.0      0.0    0.0   0.0  0.0  0.0       0.0        0.0   \n",
       "2  0.0      0.0     0.0      0.0    0.0   0.0  0.0  0.0       0.0        0.0   \n",
       "\n",
       "   ...  yes  yoruba  young  youth  youths  zero  zimbabwe  zoo  àdìre  églises  \n",
       "0  ...  0.0     0.0    0.0    0.0     0.0   0.0       0.0  0.0    0.0      0.0  \n",
       "1  ...  0.0     0.0    0.0    0.0     0.0   0.0       0.0  0.0    0.0      0.0  \n",
       "2  ...  0.0     0.0    0.0    0.0     0.0   0.0       0.0  0.0    0.0      0.0  \n",
       "\n",
       "[3 rows x 1000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Features\n",
    "\n",
    "#### Word2Vec Embeddings are based on\n",
    "<p> Continuous Bag of Words (CBOW) and Skip-gram model</p>\n",
    "<p> CBOW tends to predict the probability of a word given a context </p>\n",
    "<p>Skip-gram is the opposite, it tries to predict the context given the word</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495335, 738460)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "# lets train a word2vec on our corpus\n",
    "tokenized_tweet = df['tweets'].apply(lambda x: x.split())\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(tokenized_tweet, \n",
    "                                  size=200, # desired number of features/ independent variables\n",
    "                                  window=5, # context window size\n",
    "                                  min_count=2,\n",
    "                                  sg=1, # 1 for skip-gram model\n",
    "                                  hs=0,\n",
    "                                  negative=10, # for  negative sampling\n",
    "                                  workers=2, # no.of cores\n",
    "                                  seed=34)\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples=len(df['tweets']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('polio', 0.857085108757019),\n",
       " ('everywhere.', 0.847568690776825),\n",
       " ('poliofree', 0.8417650461196899),\n",
       " ('address', 0.8389028310775757),\n",
       " ('rose', 0.8368113040924072),\n",
       " ('head', 0.8337514400482178),\n",
       " ('fastest', 0.8324134349822998),\n",
       " ('lacking', 0.8312587141990662),\n",
       " ('foods', 0.828781008720398),\n",
       " ('wild', 0.8280803561210632)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let play with our Word2Vec model and see how it performs\n",
    "# we will specify a word and the model will pull out the most similar words from the corpus\n",
    "\n",
    "model_w2v.wv.most_similar(positive='insecurity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chains', 0.8466651439666748),\n",
       " ('chemicals', 0.8337732553482056),\n",
       " ('important.', 0.8215525150299072),\n",
       " ('toxic', 0.8157706260681152),\n",
       " ('epileptic', 0.8016002178192139),\n",
       " ('crops', 0.7981507778167725),\n",
       " ('along', 0.7925853729248047),\n",
       " ('Elimination', 0.7905422449111938),\n",
       " ('affecting', 0.7894235253334045),\n",
       " ('country,', 0.7727617025375366)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Still Playing \n",
    "model_w2v.wv.most_similar(positive='supply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.31569937 -0.24756722  0.3080822  -0.45784333 -0.21697754 -0.18244073\n",
      "  0.18578516  0.34417364  0.20207807  0.20920347 -0.09294654  0.00061577\n",
      "  0.13806966  0.11416349  0.17130673 -0.01465261 -0.05817071 -0.29717517\n",
      " -0.1930512   0.21360582  0.01235877  0.03201896 -0.0284075   0.10049054\n",
      " -0.15364872  0.0286955  -0.13232404 -0.17457804 -0.04712316 -0.43420327\n",
      "  0.10076919  0.0395518   0.23662172 -0.11921836  0.02959005  0.17109236\n",
      " -0.4761038   0.29869744 -0.12796535 -0.03052701 -0.564806    0.13947451\n",
      " -0.01580761  0.33427334  0.3322891   0.05345676 -0.06601372  0.0968715\n",
      "  0.16583315 -0.23054416  0.26838842 -0.27106705  0.08195201 -0.3275129\n",
      "  0.48243394  0.4906531  -0.19897883  0.14375235 -0.03708821 -0.28076833\n",
      "  0.13351901 -0.40291113  0.26359853 -0.16296987 -0.11484902  0.3413127\n",
      "  0.08606453  0.16285002  0.15893912 -0.15082544  0.1657976   0.13960582\n",
      "  0.30406356 -0.11988617 -0.01392338  0.10024761 -0.07802346  0.18656641\n",
      " -0.23138787  0.17118621  0.14961804  0.18088946 -0.12910591 -0.01336653\n",
      " -0.00095933  0.2269696  -0.11980116 -0.2713417   0.18863769 -0.22269373\n",
      " -0.12989444  0.3240235   0.1323278   0.34888172 -0.00877883  0.20006792\n",
      " -0.15292308  0.08547781  0.04802236 -0.27217296  0.04226024  0.48845616\n",
      " -0.02968951  0.45491236 -0.01057928 -0.30095485 -0.04283432 -0.23109193\n",
      "  0.18560864  0.12168927 -0.21648578  0.06981806  0.213218   -0.03978017\n",
      "  0.14836372 -0.0755074  -0.12603782  0.05340551 -0.50823116 -0.43968603\n",
      "  0.03079753  0.15320775 -0.51731735 -0.605358    0.2304081  -0.13715649\n",
      "  0.0119295  -0.04852427  0.10201227 -0.00357857  0.17339262 -0.04626691\n",
      " -0.0784719   0.28446975  0.01206428 -0.01458832  0.21406586  0.2369983\n",
      "  0.20158885 -0.10646603 -0.22016488  0.13132189 -0.04790879 -0.02994949\n",
      " -0.42431968 -0.08857843  0.07446472 -0.12508255 -0.00618198 -0.09417855\n",
      "  0.19029693  0.14692003 -0.05569071  0.17391928 -0.0670172  -0.03622233\n",
      " -0.05593512  0.30883253  0.04595076  0.05586319 -0.25526342 -0.5850865\n",
      "  0.16435075  0.15521044  0.25076807  0.01189049 -0.27206808 -0.07276075\n",
      "  0.20063269 -0.55920756 -0.32564652 -0.10677887 -0.28427443 -0.03996797\n",
      " -0.04382907 -0.02649966 -0.00156169  0.12350779  0.17858858 -0.39699754\n",
      "  0.1695946  -0.33391795  0.4576578   0.28027213 -0.09052089  0.14918317\n",
      "  0.06434379 -0.3814958   0.05485842  0.12434479  0.18617597 -0.05851043\n",
      "  0.14810635 -0.06614453 -0.01755338  0.09297259 -0.03182103  0.11487307\n",
      " -0.04860217  0.08769149]\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# let check the vector representation of any word from our corpus\n",
    "\n",
    "print(model_w2v['food'])\n",
    "print(len(model_w2v['food']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Document Embedding**\n",
    "\n",
    "**Doc2Vec Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledSentence(words=['Still', \"Don't\", 'know', 'where', 'my', 'dear', 'country', 'Nigeria', 'is', 'heading', 'Insecurity', 'Hunger', 'Givernment', 'failures', 'judiciary', 'not', 'trusted', 'How', 'can', 'the', 'poor', 'survive.', 'God', 'save', 'Nigeria'], tags=['tweet_0']),\n",
       " LabeledSentence(words=['WFP', 'links', 'record', 'hunger', 'levels', 'to', 'pandemic,', 'conflict,', 'climate', 'change'], tags=['tweet_1']),\n",
       " LabeledSentence(words=[':', 'WFP', 'links', 'record', 'hunger', 'levels', 'to', 'pandemic,', 'conflict,', 'climate', 'change'], tags=['tweet_3']),\n",
       " LabeledSentence(words=[':', 'Hunger', 'in', 'the', 'northeast', 'Nigeria', 'will', 'spike', 'more', 'conflicts', '…'], tags=['tweet_4']),\n",
       " LabeledSentence(words=[':', 'As', 'necessary', 'as', 'the', 'COVIDLockdown', 'is,', 'the', 'INSENSITIVE', 'Cabal', 'ruling', 'Nigeria', 'continues', 'to', 'ignore', 'the', 'rising', 'hunger/hards…'], tags=['tweet_5']),\n",
       " LabeledSentence(words=['Then', 'how', 'come', 'poverty,', 'hunger,youth', 'unemployment', 'are', 'so', 'pervasive', 'in', 'Kano', 'State?', 'How', 'come', 'Kano', 'ranks', 'poorly', 'in', 'Human', 'Development', 'Index', '(HDI).Fati,', 'you', 'need', 'to', 'come', 'out', 'of', 'your', 'bubble.', 'Your', 'father', 'Ganduje', 'is', 'one', 'of', 'the', 'worst', 'Kleptocrat', 'in', 'modern', 'Nigeria', 'political', 'history.'], tags=['tweet_6'])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc='progress-bar')\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "# to implement doc2vec, we have to labelise or tag each tokenized with uniqueIds\n",
    "\n",
    "def add_labels(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(LabeledSentence(s, ['tweet_' + str(i)]))\n",
    "    return output\n",
    "\n",
    "labeled_tweets = add_labels(tokenized_tweet) # label all the tweets\n",
    "\n",
    "labeled_tweets[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1339/1339 [00:00<00:00, 335554.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Let train a doc2vvec model\n",
    "model_d2v = gensim.models.Doc2Vec(dm=1, # for distributed memory\n",
    "                                 dm_mean=1, # fo  using mean of th context word vectors\n",
    "                                 size=200, # no of features\n",
    "                                 window=5, #width of the context window\n",
    "                                 negative=7, # if > 0 then negative sampling will be used\n",
    "                                 min_count=3, # ignores all words with total frequency lower than 3\n",
    "                                 workers=3, # no of cores\n",
    "                                 alpha=.1, #learning rate\n",
    "                                 seed=23)\n",
    "\n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n",
    "\n",
    "model_d2v.train(labeled_tweets, total_examples=len(df['tweets']), epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1339, 200)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing doc2vec feature set\n",
    "docvev_arrays = np.zeros((len(tokenized_tweet), 200))\n",
    "for i in range(len(df)):\n",
    "    docvev_arrays[i,:] = model_d2v.docvecs[i].reshape((1, 200))\n",
    "    \n",
    "docvec_df = pd.DataFrame(docvev_arrays)\n",
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vc features\n",
    "train_w2v = wordvec_df.iloc[:31962,:]\n",
    "test_w2v = wordvec_df.iloc[31962:,:]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_w2v, train['label'], random_state=42, test_size=.2)\n",
    "\n",
    "lreg.fit(X_train, y_train)\n",
    "pred = lreg.predict_proba(X_val)\n",
    "pred_int = pred[:,1] >= .3\n",
    "pred_int = pred_int.astype(np.int)\n",
    "\n",
    "print(metrics.f1_score(pred_int, y_val)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
