{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Impact_of_Covid_19_on_Food_Quantity_in_Africa(Data_Collection).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS_503x9jCUP"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from collections import Counter\n",
        "from datetime import datetime, date, time, timedelta\n",
        "\n",
        "import tweepy\n",
        "from tweepy.streaming import StreamListener\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy import Stream\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "\n",
        "import preprocessor as p\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# to view all columns\n",
        "pd.set_option(\"display.max.columns\", None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNKZIapp0QWT"
      },
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "from pathlib import Path \n",
        "env_path = Path('.') / '.env'\n",
        "load_dotenv(dotenv_path=env_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6idg-Hi33ub2"
      },
      "source": [
        "class tweetsearch():\n",
        "    '''\n",
        "    This is a basic class to search and download twitter data.\n",
        "    You can build up on it to extend the functionalities for more \n",
        "    sophisticated analysis\n",
        "    '''\n",
        "    def __init__(self, cols=None,auth=None):\n",
        "        #\n",
        "        if not cols is None:\n",
        "            self.cols = cols\n",
        "        else:\n",
        "            self.cols = ['id', 'created_at', 'source', 'original_text','clean_text', \n",
        "                    'sentiment','polarity','subjectivity', 'lang',\n",
        "                    'favorite_count', 'retweet_count', 'original_author',   \n",
        "                    'possibly_sensitive', 'hashtags',\n",
        "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
        "            \n",
        "        if auth is None:\n",
        "            \n",
        "            #Variables that contains the user credentials to access Twitter API \n",
        "            consumer_key = os.environ.get('TWITTER_API_KEY')\n",
        "            consumer_secret = os.environ.get('TWITTER_API_SECRET')\n",
        "            access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
        "            access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
        "            \n",
        "\n",
        "\n",
        "            #This handles Twitter authetification and the connection to Twitter Streaming API\n",
        "            auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "            auth.set_access_token(access_token, access_token_secret)\n",
        "            \n",
        "\n",
        "        #            \n",
        "        self.auth = auth\n",
        "        self.api = tweepy.API(auth,wait_on_rate_limit=True) \n",
        "        self.filtered_tweet = ''\n",
        "            \n",
        "\n",
        "    def clean_tweets(self, twitter_text):\n",
        "\n",
        "        #use pre processor\n",
        "        tweet = p.clean(twitter_text)\n",
        "\n",
        "         #HappyEmoticons\n",
        "        emoticons_happy = set([\n",
        "            ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "            ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "            '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "            'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "            '<3'\n",
        "            ])\n",
        "\n",
        "        # Sad Emoticons\n",
        "        emoticons_sad = set([\n",
        "            ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "            ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "            ':c', ':{', '>:\\\\', ';('\n",
        "            ])\n",
        "\n",
        "        #Emoji patterns\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                 u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                 u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                 u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                 u\"\\U00002702-\\U000027B0\"\n",
        "                 u\"\\U000024C2-\\U0001F251\"\n",
        "                 \"]+\", flags=re.UNICODE)\n",
        "\n",
        "        #combine sad and happy emoticons\n",
        "        emoticons = emoticons_happy.union(emoticons_sad)\n",
        "\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        word_tokens = nltk.word_tokenize(tweet)\n",
        "        #after tweepy preprocessing the colon symbol left remain after      \n",
        "        #removing mentions\n",
        "        tweet = re.sub(r':', '', tweet)\n",
        "        tweet = re.sub(r'‚Ä¶', '', tweet)\n",
        "\n",
        "        #replace consecutive non-ASCII characters with a space\n",
        "        tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
        "\n",
        "        #remove emojis from tweet\n",
        "        tweet = emoji_pattern.sub(r'', tweet)\n",
        "\n",
        "        #filter using NLTK library append it to a string\n",
        "        filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "        #looping through conditions\n",
        "        filtered_tweet = []    \n",
        "        for w in word_tokens:\n",
        "        #check tokens against stop words , emoticons and punctuations\n",
        "            if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
        "                filtered_tweet.append(w)\n",
        "\n",
        "        return ' '.join(filtered_tweet)            \n",
        "\n",
        "    def get_tweets(self, keyword, csvfile=None):\n",
        "        \n",
        "        \n",
        "        df = pd.DataFrame(columns=self.cols)\n",
        "        \n",
        "\n",
        "        #page attribute in tweepy.cursor and iteration\n",
        "        for page in tweepy.Cursor(self.api.search, q=keyword,count=100, include_rts=False,tweet_mode='extended').pages():\n",
        "\n",
        "            # the you receive from the Twitter API is in a JSON format and has quite an amount of information attached\n",
        "            for status in page:\n",
        "                \n",
        "                new_entry = []\n",
        "                status = status._json\n",
        "                \n",
        "                #if this tweet is a retweet update retweet count\n",
        "                if status['created_at'] in df['created_at'].values:\n",
        "                    i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
        "                    #\n",
        "                    cond1 = status['favorite_count'] != df.at[i, 'favorite_count']\n",
        "                    cond2 = status['retweet_count'] != df.at[i, 'retweet_count']\n",
        "                    if cond1 or cond2:\n",
        "                        df.at[i, 'favorite_count'] = status['favorite_count']\n",
        "                        df.at[i, 'retweet_count'] = status['retweet_count']\n",
        "                    continue\n",
        "\n",
        "                #calculate sentiment\n",
        "                filtered_tweet = self.clean_tweets(status['full_text'])\n",
        "                blob = TextBlob(filtered_tweet)\n",
        "                Sentiment = blob.sentiment     \n",
        "                polarity = Sentiment.polarity\n",
        "                subjectivity = Sentiment.subjectivity\n",
        "\n",
        "                new_entry += [status['id'], status['created_at'],\n",
        "                              status['source'], status['full_text'], filtered_tweet, \n",
        "                              Sentiment,polarity,subjectivity, status['lang'],\n",
        "                              status['favorite_count'], status['retweet_count']]\n",
        "\n",
        "                new_entry.append(status['user']['screen_name'])\n",
        "\n",
        "                try:\n",
        "                    is_sensitive = status['possibly_sensitive']\n",
        "                except KeyError:\n",
        "                    is_sensitive = None\n",
        "\n",
        "                new_entry.append(is_sensitive)\n",
        "\n",
        "                hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
        "                new_entry.append(hashtags) #append the hashtags\n",
        "\n",
        "                #\n",
        "                mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
        "                new_entry.append(mentions) #append the user mentions\n",
        "\n",
        "                try:\n",
        "                    xyz = status['place']['bounding_box']['coordinates']\n",
        "                    coordinates = [coord for loc in xyz for coord in loc]\n",
        "                except TypeError:\n",
        "                    coordinates = None\n",
        "                #\n",
        "                new_entry.append(coordinates)\n",
        "\n",
        "                try:\n",
        "                    location = status['user']['location']\n",
        "                except TypeError:\n",
        "                    location = ''\n",
        "                #\n",
        "                new_entry.append(location)\n",
        "\n",
        "                #now append a row to the dataframe\n",
        "                single_tweet_df = pd.DataFrame([new_entry], columns=self.cols)\n",
        "                df = df.append(single_tweet_df, ignore_index=True)\n",
        "\n",
        "        #\n",
        "        df['timestamp'] = df.created_at.map(pd.Timestamp)\n",
        "        df = df.sort_values('timestamp').set_index('timestamp')\n",
        "        df = df.drop('id',axis=1)\n",
        "        \n",
        "        if not csvfile is None:\n",
        "            #save it to file\n",
        "            df.to_csv(csvfile,mode='a', index=True, encoding=\"utf-8\")\n",
        "            \n",
        "\n",
        "        return df \n",
        "\n",
        "    def get_timeline(self, username, csvfile=None):\n",
        "        \n",
        "        \n",
        "        df = pd.DataFrame(columns=self.cols)\n",
        "        \n",
        "\n",
        "        #page attribute in tweepy.cursor and iteration\n",
        "        for page in tweepy.Cursor(self.api.user_timeline, screen_name=username, count=100, include_rts=False,tweet_mode='extended', since = '2019-09-01').pages():\n",
        "\n",
        "            # the you receive from the Twitter API is in a JSON format and has quite an amount of information attached\n",
        "            for status in page:\n",
        "                \n",
        "                new_entry = []\n",
        "                status = status._json\n",
        "                \n",
        "                #if this tweet is a retweet update retweet count\n",
        "                if status['created_at'] in df['created_at'].values:\n",
        "                    i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
        "                    #\n",
        "                    cond1 = status['favorite_count'] != df.at[i, 'favorite_count']\n",
        "                    cond2 = status['retweet_count'] != df.at[i, 'retweet_count']\n",
        "                    if cond1 or cond2:\n",
        "                        df.at[i, 'favorite_count'] = status['favorite_count']\n",
        "                        df.at[i, 'retweet_count'] = status['retweet_count']\n",
        "                    continue\n",
        "\n",
        "                #calculate sentiment\n",
        "                filtered_tweet = self.clean_tweets(status['full_text'])\n",
        "                blob = TextBlob(filtered_tweet)\n",
        "                Sentiment = blob.sentiment     \n",
        "                polarity = Sentiment.polarity\n",
        "                subjectivity = Sentiment.subjectivity\n",
        "\n",
        "                new_entry += [status['id'], status['created_at'],\n",
        "                              status['source'], status['full_text'], filtered_tweet, \n",
        "                              Sentiment,polarity,subjectivity, status['lang'],\n",
        "                              status['favorite_count'], status['retweet_count']]\n",
        "\n",
        "                new_entry.append(status['user']['screen_name'])\n",
        "\n",
        "                try:\n",
        "                    is_sensitive = status['possibly_sensitive']\n",
        "                except KeyError:\n",
        "                    is_sensitive = None\n",
        "\n",
        "                new_entry.append(is_sensitive)\n",
        "\n",
        "                hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
        "                new_entry.append(hashtags) #append the hashtags\n",
        "\n",
        "                #\n",
        "                mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
        "                new_entry.append(mentions) #append the user mentions\n",
        "\n",
        "                try:\n",
        "                    xyz = status['place']['bounding_box']['coordinates']\n",
        "                    coordinates = [coord for loc in xyz for coord in loc]\n",
        "                except TypeError:\n",
        "                    coordinates = None\n",
        "                #\n",
        "                new_entry.append(coordinates)\n",
        "\n",
        "                try:\n",
        "                    location = status['user']['location']\n",
        "                except TypeError:\n",
        "                    location = ''\n",
        "                #\n",
        "                new_entry.append(location)\n",
        "\n",
        "                #now append a row to the dataframe\n",
        "                single_tweet_df = pd.DataFrame([new_entry], columns=self.cols)\n",
        "                df = df.append(single_tweet_df, ignore_index=True)\n",
        "\n",
        "        #\n",
        "        df['timestamp'] = df.created_at.map(pd.Timestamp)\n",
        "        df = df.sort_values('timestamp').set_index('timestamp')\n",
        "        df = df.drop('id',axis=1)\n",
        "\n",
        "        if not csvfile is None:\n",
        "            #save it to file\n",
        "            df.to_csv(csvfile, index=True,mode='a', encoding=\"utf-8\")\n",
        "            \n",
        "\n",
        "        return df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eefql-FkDZqg"
      },
      "source": [
        "additional = ['corona','covid','covid-19']\n",
        "list_of_places_one = ['Kenya','Nairobi','Kisumu','Mombasa']\n",
        "list_of_places_two = ['Nigeria','Lagos']\n",
        "list_of_places_three = ['South Africa','Johannesburg','Cape Town']\n",
        "all_list_of_places = [list_of_places_one,list_of_places_two,list_of_places_three]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kScUTqAc7-YT"
      },
      "source": [
        "list_of_keywords = ['Covid-19 Food response','Food Supply','Food Scarcity','Food Supply Chain','Food Availability','Food Provision','Food Distribution','Food Rations','Food Web','Food Network','Food Deficit','Hunger','Food Poverty','Food Security','Food Insecurity','Food Price','Starvation','Starving','Food Shortage','Food Safety','Cost of Food','Food Costs','Price of Food', 'Cost of Foodstuffs','Foodstuffs Costs','Price of Foodstuffs','Cost of Grocery','Grocery Costs', 'Price of Grocery']\n",
        "\n",
        "def food_quantity(all_list_of_places):\n",
        "    for list_of_places in all_list_of_places:\n",
        "        if list_of_places == list_of_places_one:\n",
        "            file_name = 'KenyaTweets.csv'\n",
        "        elif list_of_places == list_of_places_two:\n",
        "            file_name = 'NigeriaTweets.csv'\n",
        "        else:\n",
        "            file_name = 'SouthAfricaTweets.csv'\n",
        "            \n",
        "        for place in list_of_places:\n",
        "            for item in additional:\n",
        "                keywords = ['Food',place,item]\n",
        "                final_keywords = \" AND \".join(keywords)\n",
        "                try:\n",
        "                    ts = tweetsearch() \n",
        "                    df = ts.get_tweets(final_keywords, csvfile=file_name)\n",
        "                    print(final_keywords)\n",
        "                except tweepy.TweepError:\n",
        "                    continue\n",
        "\n",
        "        for keyword in list_of_keywords:\n",
        "            for place in list_of_places:\n",
        "                keywords = [keyword,place]\n",
        "                final_keywords = \" AND \".join(keywords)\n",
        "                try:\n",
        "                    ts = tweetsearch() \n",
        "                    df = ts.get_tweets(final_keywords, csvfile=file_name)\n",
        "                    print(final_keywords)\n",
        "                except tweepy.TweepError:\n",
        "                    continue\n",
        "\n",
        "                for item in additional:\n",
        "                    keywords = [keyword,place,item]\n",
        "                    final_keywords = \" AND \".join(keywords)\n",
        "                    try:\n",
        "                        ts = tweetsearch() \n",
        "                        df = ts.get_tweets(final_keywords, csvfile=file_name)\n",
        "                        print(final_keywords)\n",
        "                    except tweepy.TweepError:\n",
        "                        continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9eVIaCfDZqn"
      },
      "source": [
        "food_quantity(all_list_of_places)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjACn3UsYFLK"
      },
      "source": [
        "Kenya_Tweets = pd.read_csv('KenyaTweets.csv', low_memory = False)\n",
        "Nigeria_Tweets = pd.read_csv('NigeriaTweets.csv', low_memory = False)\n",
        "South_Africa_Tweets = pd.read_csv('SouthAfricaTweets.csv', low_memory = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO91aunBgR8a"
      },
      "source": [
        "def list_of_users(dataset):\n",
        "    users = list(dataset['original_author'].unique())\n",
        "    return users"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNNECEMJDZqx"
      },
      "source": [
        "kenyan_users =  list_of_users(Kenya_Tweets)\n",
        "nigerian_users = list_of_users(Nigeria_Tweets)\n",
        "south_african_users = list_of_users(South_Africa_Tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6wi-PY1fF5J"
      },
      "source": [
        "all_users = [kenyan_users, nigerian_users, south_african_users]\n",
        "\n",
        "def user_tweets(all_users):\n",
        "    for users in all_users:\n",
        "        if users == kenyan_users:\n",
        "            file_name = 'KenyanTweets.csv'\n",
        "        elif users == nigerian_users:\n",
        "            file_name = 'NigerianTweets.csv'\n",
        "        else:\n",
        "            file_name = 'SouthAfricanTweets.csv'\n",
        "        \n",
        "        for user in users:\n",
        "            try:\n",
        "                ts = tweetsearch()\n",
        "                df = ts.get_timeline(user, csvfile=file_name)\n",
        "                #Helps keep track of usernames\n",
        "                print(user)\n",
        "\n",
        "            except tweepy.TweepError:\n",
        "                continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO-5uy2pDZq2"
      },
      "source": [
        "user_tweets(all_users)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}